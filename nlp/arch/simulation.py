import numpy as np

# ============================================================================
# SEQ2SEQ + ATTENTION MEKANIZMASI SÄ°MÃœLASYONU
# ============================================================================
# Bu dosya, gerÃ§ek bir eÄŸitim yapmadan seq2seq + attention mekanizmasÄ±nÄ±
# adÄ±m adÄ±m gÃ¶stermek iÃ§in tasarlanmÄ±ÅŸtÄ±r.
# ============================================================================

def softmax(x):
    """
    Softmax fonksiyonu: girdi vektÃ¶rÃ¼nÃ¼ olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na Ã§evirir
    exp(x_i) / sum(exp(x))
    """
    # Numerik stabilite iÃ§in max deÄŸeri Ã§Ä±karÄ±yoruz
    exp_x = np.exp(x - np.max(x))
    return exp_x / exp_x.sum()

def create_word_embeddings(words, embedding_dim=4):
    """
    Her kelime iÃ§in rastgele embedding vektÃ¶rÃ¼ oluÅŸturur
    GerÃ§ek uygulamada bu vektÃ¶rler Ã¶ÄŸrenilir, burada rastgele atÄ±yoruz
    """
    embeddings = {}
    for word in words:
        # Her kelime iÃ§in embedding_dim boyutunda rastgele vektÃ¶r
        embeddings[word] = np.random.randn(embedding_dim)
    return embeddings

def encoder_step(word_embedding, prev_hidden):
    """
    Encoder'Ä±n bir adÄ±mÄ±nÄ± simÃ¼le eder
    GerÃ§ek RNN/LSTM'de karmaÅŸÄ±k hesaplamalar var, burada basitleÅŸtirilmiÅŸ
    """
    # Basit linear transformation simÃ¼lasyonu
    # GerÃ§ekte: h_t = tanh(W_h * h_{t-1} + W_x * x_t + b)
    hidden_size = prev_hidden.shape[0]
    
    # Rastgele aÄŸÄ±rlÄ±k matrisleri (normalde eÄŸitilir)
    W_h = np.random.randn(hidden_size, hidden_size) * 0.1
    W_x = np.random.randn(hidden_size, word_embedding.shape[0]) * 0.1
    
    # Yeni hidden state hesaplama
    new_hidden = np.tanh(W_h @ prev_hidden + W_x @ word_embedding)
    
    return new_hidden

def calculate_attention(decoder_hidden, encoder_hiddens, encoder_words):
    """
    Attention mekanizmasÄ±nÄ± hesaplar
    1. Her encoder hidden state ile decoder hidden state arasÄ±nda dot product
    2. Softmax ile attention aÄŸÄ±rlÄ±klarÄ±nÄ± hesapla
    3. Context vector oluÅŸtur
    """
    print("\n    --- Attention Hesaplama ---")
    
    # 1. Attention skorlarÄ±nÄ± hesapla (dot product)
    attention_scores = []
    for i, enc_hidden in enumerate(encoder_hiddens):
        # Dot product: decoder_hidden Â· encoder_hidden
        score = np.dot(decoder_hidden, enc_hidden)
        attention_scores.append(score)
        print(f"    '{encoder_words[i]}' kelimesi iÃ§in attention skoru: {score:.4f}")
    
    attention_scores = np.array(attention_scores)
    
    # 2. Softmax ile normalize et (olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±)
    attention_weights = softmax(attention_scores)
    
    print("\n    --- Attention AÄŸÄ±rlÄ±klarÄ± (Softmax sonrasÄ±) ---")
    for i, word in enumerate(encoder_words):
        print(f"    '{word}': {attention_weights[i]:.4f} ({attention_weights[i]*100:.2f}%)")
    
    # 3. Context vector oluÅŸtur (weighted sum)
    # Context = sum(attention_weight_i * encoder_hidden_i)
    context_vector = np.zeros_like(encoder_hiddens[0])
    for i, enc_hidden in enumerate(encoder_hiddens):
        context_vector += attention_weights[i] * enc_hidden
    
    print(f"\n    Context vector oluÅŸturuldu: shape={context_vector.shape}")
    
    return context_vector, attention_weights

def decoder_step(prev_word_embedding, prev_hidden, context_vector):
    """
    Decoder'Ä±n bir adÄ±mÄ±nÄ± simÃ¼le eder
    Context vector ile decoder hidden state birleÅŸtirilerek yeni output Ã¼retilir
    """
    hidden_size = prev_hidden.shape[0]
    
    # Rastgele aÄŸÄ±rlÄ±k matrisleri (normalde eÄŸitilir)
    W_h = np.random.randn(hidden_size, hidden_size) * 0.1
    W_x = np.random.randn(hidden_size, prev_word_embedding.shape[0]) * 0.1
    W_c = np.random.randn(hidden_size, context_vector.shape[0]) * 0.1
    
    # Yeni hidden state: context vector da dahil edilir
    # h_t = tanh(W_h * h_{t-1} + W_x * x_t + W_c * context + b)
    new_hidden = np.tanh(
        W_h @ prev_hidden + 
        W_x @ prev_word_embedding + 
        W_c @ context_vector
    )
    
    return new_hidden

def select_output_word(decoder_hidden, vocab, vocab_embeddings):
    """
    Decoder hidden state'e gÃ¶re en uygun kelimeyi seÃ§er
    GerÃ§ekte: softmax(W * hidden + b) ile olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± hesaplanÄ±r
    Burada basitÃ§e: hidden state ile vocab embeddings arasÄ±nda similarity
    """
    # Decoder hidden state boyutunu embedding boyutuna project et
    # GerÃ§ek uygulamada bu bir Ã¶ÄŸrenilen linear layer olurdu
    hidden_size = decoder_hidden.shape[0]
    
    # Ä°lk kelime embedding'inin boyutunu al
    embedding_dim = vocab_embeddings[vocab[0]].shape[0]
    
    # Projection matrisi (hidden_size -> embedding_dim)
    W_projection = np.random.randn(embedding_dim, hidden_size) * 0.1
    
    # Decoder hidden'Ä± embedding boyutuna project et
    projected_hidden = W_projection @ decoder_hidden
    
    scores = {}
    for word in vocab:
        # Her kelime embedding'i ile projected hidden state arasÄ±nda benzerlik (dot product)
        score = np.dot(projected_hidden, vocab_embeddings[word])
        scores[word] = score
    
    # En yÃ¼ksek skora sahip kelimeyi seÃ§
    selected_word = max(scores, key=scores.get)
    return selected_word, scores

# ============================================================================
# ANA SÄ°MÃœLASYON
# ============================================================================

def run_simulation():
    """
    Seq2seq + Attention mekanizmasÄ±nÄ±n tam simÃ¼lasyonu
    """
    print("=" * 80)
    print("SEQ2SEQ + ATTENTION MEKANIZMASI SÄ°MÃœLASYONU")
    print("=" * 80)
    
    # -------------------------------------------------------------------------
    # 1. GÄ°RDÄ° CÃœMLESI VE HEDEF CÃœMLE
    # -------------------------------------------------------------------------
    input_sentence = "Ben okula gidiyorum"
    input_words = input_sentence.split()
    
    # Hedef cÃ¼mle (decoder'Ä±n Ã¼retmesi gereken)
    target_sentence = "I am going to school"
    target_words = target_sentence.split()
    
    print(f"\nğŸ“ Girdi CÃ¼mlesi (TÃ¼rkÃ§e): {input_sentence}")
    print(f"ğŸ¯ Hedef CÃ¼mle (Ä°ngilizce): {target_sentence}")
    print(f"\nEncoder'a giren kelimeler: {input_words}")
    
    # -------------------------------------------------------------------------
    # 2. KELÄ°ME EMBEDDÄ°NGLERÄ° OLUÅTUR
    # -------------------------------------------------------------------------
    print("\n" + "-" * 80)
    print("ADIM 1: KELÄ°ME EMBEDDÄ°NGLERÄ° OLUÅTURMA")
    print("-" * 80)
    
    embedding_dim = 4  # Her kelimenin vektÃ¶r boyutu
    hidden_size = 6    # Hidden state boyutu
    
    # TÃ¼m kelimelerin listesi (girdi + hedef + Ã¶zel tokenlar)
    all_words = input_words + target_words + ["<START>", "<END>"]
    
    # Her kelime iÃ§in embedding vektÃ¶rÃ¼
    word_embeddings = create_word_embeddings(all_words, embedding_dim)
    
    print(f"\nToplam {len(all_words)} kelime iÃ§in {embedding_dim} boyutlu embedding oluÅŸturuldu")
    for word in input_words:
        print(f"  '{word}': {word_embeddings[word]}")
    
    # -------------------------------------------------------------------------
    # 3. ENCODER Ä°ÅLEMÄ°
    # -------------------------------------------------------------------------
    print("\n" + "-" * 80)
    print("ADIM 2: ENCODER Ä°ÅLEMÄ°")
    print("-" * 80)
    print("Encoder, girdi cÃ¼mlesindeki her kelimeyi sÄ±rayla iÅŸler")
    print("ve her adÄ±mda bir hidden state Ã¼retir.\n")
    
    # Encoder'Ä±n baÅŸlangÄ±Ã§ hidden state'i (sÄ±fÄ±rlarla baÅŸla)
    encoder_hidden = np.zeros(hidden_size)
    encoder_hiddens = []  # Her adÄ±mdaki hidden state'leri sakla
    
    for i, word in enumerate(input_words):
        print(f"Encoder AdÄ±m {i+1}: '{word}' kelimesi iÅŸleniyor...")
        
        # Kelimenin embedding'ini al
        word_embedding = word_embeddings[word]
        print(f"  Embedding: {word_embedding}")
        
        # Encoder step
        encoder_hidden = encoder_step(word_embedding, encoder_hidden)
        encoder_hiddens.append(encoder_hidden)
        
        print(f"  Yeni hidden state: {encoder_hidden}")
        print()
    
    print(f"âœ“ Encoder tamamlandÄ±. {len(encoder_hiddens)} adet hidden state oluÅŸturuldu.")
    
    # -------------------------------------------------------------------------
    # 4. DECODER Ä°ÅLEMÄ° (ATTENTION Ä°LE)
    # -------------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("ADIM 3: DECODER Ä°ÅLEMÄ° (ATTENTION MEKANIZMASI Ä°LE)")
    print("=" * 80)
    print("Decoder, her adÄ±mda:")
    print("  1. Attention mekanizmasÄ± ile encoder Ã§Ä±ktÄ±larÄ±na odaklanÄ±r")
    print("  2. Context vector oluÅŸturur")
    print("  3. Yeni bir kelime Ã¼retir")
    print("=" * 80)
    
    # Decoder'Ä±n baÅŸlangÄ±Ã§ hidden state'i (encoder'Ä±n son hidden state'i)
    decoder_hidden = encoder_hiddens[-1]
    
    # BaÅŸlangÄ±Ã§ tokeni
    prev_word = "<START>"
    
    # Ã‡Ä±ktÄ± vocabulary (decoder'Ä±n Ã¼retebileceÄŸi kelimeler)
    output_vocab = target_words + ["<END>"]
    
    # Ãœretilen kelimeler
    generated_words = []
    
    # Maksimum decoder adÄ±mÄ± (sonsuz dÃ¶ngÃ¼yÃ¼ Ã¶nlemek iÃ§in)
    max_decoder_steps = 10
    
    for step in range(max_decoder_steps):
        print(f"\n{'='*80}")
        print(f"DECODER ADIM {step+1}")
        print(f"{'='*80}")
        print(f"Ã–nceki kelime: '{prev_word}'")
        
        # Ã–nceki kelimenin embedding'i
        prev_word_embedding = word_embeddings[prev_word]
        
        # -----------------------------------------------------------------------
        # ATTENTION MEKANIZMASI
        # -----------------------------------------------------------------------
        print("\nğŸ” Attention mekanizmasÄ± devreye giriyor...")
        context_vector, attention_weights = calculate_attention(
            decoder_hidden, 
            encoder_hiddens, 
            input_words
        )
        
        # -----------------------------------------------------------------------
        # DECODER STEP
        # -----------------------------------------------------------------------
        print("\nâš™ï¸  Decoder hidden state gÃ¼ncelleniyor...")
        decoder_hidden = decoder_step(prev_word_embedding, decoder_hidden, context_vector)
        print(f"Yeni decoder hidden state: {decoder_hidden}")
        
        # -----------------------------------------------------------------------
        # Ã‡IKTI KELÄ°MESÄ° SEÃ‡ME
        # -----------------------------------------------------------------------
        print("\nğŸ² Ã‡Ä±ktÄ± kelimesi seÃ§iliyor...")
        selected_word, scores = select_output_word(
            decoder_hidden, 
            output_vocab, 
            word_embeddings
        )
        
        print("TÃ¼m kelimeler iÃ§in skorlar:")
        for word, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):
            marker = " â† SEÃ‡Ä°LDÄ°" if word == selected_word else ""
            print(f"  '{word}': {score:.4f}{marker}")
        
        print(f"\nâœ¨ Ãœretilen kelime: '{selected_word}'")
        
        # Ãœretilen kelimeyi kaydet
        generated_words.append(selected_word)
        
        # EÄŸer <END> tokeni Ã¼retildiyse dur
        if selected_word == "<END>":
            print("\nğŸ <END> tokeni Ã¼retildi. Decoder durduruluyor.")
            break
        
        # Sonraki adÄ±m iÃ§in bu kelimeyi kullan
        prev_word = selected_word
    
    # -------------------------------------------------------------------------
    # 5. SONUÃ‡LARI GÃ–STER
    # -------------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("SÄ°MÃœLASYON SONUÃ‡LARI")
    print("=" * 80)
    print(f"\nğŸ“¥ Girdi CÃ¼mlesi    : {input_sentence}")
    print(f"ğŸ“¤ Ãœretilen CÃ¼mle   : {' '.join(generated_words)}")
    print(f"ğŸ¯ Hedef CÃ¼mle      : {target_sentence}")
    
    print("\n" + "=" * 80)
    print("AÃ‡IKLAMA")
    print("=" * 80)
    print("""
Bu simÃ¼lasyonda:
- Encoder, TÃ¼rkÃ§e cÃ¼mleyi kelime kelime iÅŸledi ve her kelime iÃ§in bir 
  hidden state Ã¼retti.
  
- Decoder, her adÄ±mda:
  * Attention mekanizmasÄ± ile encoder'Ä±n hangi kelimesine odaklanacaÄŸÄ±na karar verdi
  * Dot product ile attention skorlarÄ± hesaplandÄ±
  * Softmax ile bu skorlar olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na Ã§evrildi
  * Weighted sum ile context vector oluÅŸturuldu
  * Bu context vector kullanÄ±larak yeni bir kelime Ã¼retildi

- Attention aÄŸÄ±rlÄ±klarÄ±, decoder'Ä±n her adÄ±mda encoder'Ä±n hangi kelimesine
  ne kadar Ã¶nem verdiÄŸini gÃ¶sterir.

NOT: Bu bir simÃ¼lasyondur. GerÃ§ek bir seq2seq modelinde aÄŸÄ±rlÄ±klar
     eÄŸitim sÄ±rasÄ±nda Ã¶ÄŸrenilir ve anlamlÄ± Ã§eviriler Ã¼retir.
     Burada aÄŸÄ±rlÄ±klar rastgele olduÄŸu iÃ§in Ã§Ä±ktÄ± da rastgeledir.
    """)
    print("=" * 80)

# ============================================================================
# PROGRAMI Ã‡ALIÅTIR
# ============================================================================

if __name__ == "__main__":
    # Random seed (her Ã§alÄ±ÅŸtÄ±rmada aynÄ± sonucu gÃ¶rmek iÃ§in)
    np.random.seed(42)
    
    # SimÃ¼lasyonu baÅŸlat
    run_simulation()

